{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개발 환경 및 라이브러리 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "# OS 정보 출력\n",
    "print(f\"OS: {os.name}\") \n",
    "print(f\"Platform: {platform.system()} {platform.release()}\") \n",
    "\n",
    "# Python 버전 출력\n",
    "print(f\"Python Version: {sys.version}\") \n",
    "print(f\"Python Version (short): {platform.python_version()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# 확인할 라이브러리 목록\n",
    "libraries = [\n",
    "    \"os\", \"time\", \"gc\", \"pickle\", \"joblib\", \"collections\",  # 기본 라이브러리\n",
    "    \"numpy\", \"pandas\",  # 데이터 처리\n",
    "    \"matplotlib\", \"tqdm\",  # 시각화 및 진행바\n",
    "    \"sklearn\", \"skopt\",  # 머신러닝 관련\n",
    "    \"xgboost\", \"catboost\",  # 머신러닝 모델\n",
    "    \"torch\", \"transformers\"  # 딥러닝 관련\n",
    "]\n",
    "\n",
    "# 버전 확인 함수\n",
    "def get_library_versions(libs):\n",
    "    versions = {}\n",
    "    for lib in sorted(libs):\n",
    "        try:\n",
    "            module = importlib.import_module(lib)\n",
    "            version = getattr(module, '__version__', 'Unknown')\n",
    "            versions[lib] = version\n",
    "        except ImportError:\n",
    "            versions[lib] = \"Not Installed\"\n",
    "        except AttributeError:\n",
    "            versions[lib] = \"No Version Info\"\n",
    "\n",
    "    return versions\n",
    "\n",
    "versions = get_library_versions(libraries)\n",
    "\n",
    "# Python 버전 출력\n",
    "print(f\"Python Version: {sys.version.split()[0]}\")\n",
    "\n",
    "# 라이브러리 버전 출력\n",
    "for lib, version in versions.items():\n",
    "    print(f\"{lib}: {version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gc\n",
    "import pickle\n",
    "import joblib\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "plt.rc('font', family='NanumGothic')  # 한글 깨짐 방지 (Linux 사용자)\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 마이너스 기호 깨짐 방지\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 디렉토리 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"./datasets/\"               # raw 데이터 & 전처리 완료된 데이터 & sample submission이 저장된 디렉토리\n",
    "dir_to_submit = \"./submit/\"       # submission 파일들이 저장될 디렉토리\n",
    "dir_models = './models/'          # model.pkl 파일들이 저장될 디렉토리\n",
    "\n",
    "# 폴더가 없다면 생성\n",
    "os.makedirs(dir, exist_ok=True)\n",
    "os.makedirs(dir_to_submit, exist_ok = True)\n",
    "os.makedirs(dir_models, exist_ok = True)\n",
    "\n",
    "print(f\"폴더 '{dir}, {dir_to_submit}, {dir_models}'생성\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "for pkg in sorted(pkg_resources.working_set, key=lambda x: x.project_name.lower()):\n",
    "    print(f\"{pkg.project_name}=={pkg.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(dir + 'train.csv', encoding='utf-8').drop(columns=['ID'])\n",
    "test = pd.read_csv(dir + 'test.csv', encoding='utf-8').drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. 변수 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. `배아 생성 주요 이유` 컬럼\n",
    "- 현재 시술용, 난자 저장용, 배아 저장용, 기증용 칼럼을 만들어 해당 값이 존재하면 1, 존재하지 않으면 0\n",
    "- 기존 칼럼은 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_embryo_reason(train_df, test_df):\n",
    "    # 데이터 복사\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "\n",
    "    # 새로운 컬럼 리스트\n",
    "    new_columns = ['현재 시술용', '난자 저장용', '배아 저장용', '기증용']\n",
    "\n",
    "    # 각 컬럼을 0으로 초기화\n",
    "    if '배아 생성 주요 이유' in train_df.columns:\n",
    "        for col in new_columns:\n",
    "            train_df[col] = 0\n",
    "            test_df[col] = 0\n",
    "\n",
    "\n",
    "        for index, value in train_df['배아 생성 주요 이유'].items():\n",
    "            if pd.notna(value):  # 결측값이 아닌 경우만 처리\n",
    "                for col in new_columns:\n",
    "                    if col in str(value):  # 문자열 변환 후 확인\n",
    "                        train_df.at[index, col] = 1\n",
    "\n",
    "\n",
    "        for index, value in test_df['배아 생성 주요 이유'].items():\n",
    "            if pd.notna(value):  # 결측값이 아닌 경우만 처리\n",
    "                for col in new_columns:\n",
    "                    if col in str(value):  # 문자열 변환 후 확인\n",
    "                        test_df.at[index, col] = 1\n",
    "\n",
    "    # 기존 '배아 생성 주요 이유' 컬럼 삭제\n",
    "    train_df.drop(columns=['배아 생성 주요 이유'], inplace=True, errors='ignore')\n",
    "    test_df.drop(columns=['배아 생성 주요 이유'], inplace=True, errors='ignore')\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "# 사용 예시\n",
    "train, test = process_embryo_reason(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. 이상치 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. 배아 관련 이상치(IVF)\n",
    "- “`특정 시술 유형`”에 ICSI가 포함되지 않는데 “미세”가 포함된 변수 값이 모두 0이 아닌 행 삭제\n",
    "- “`이식된 배아 수`” == 0인데 “`배아 이식 경과일`” > 0 인 행 삭제\n",
    "- “`동결 배아 사용 여부`” == 0인데 “`해동된 배아 수`” > 0인 행 삭제\n",
    "- “`동결 배아 사용 여부`” == 0 & “`배아 해동 경과일`”이 결측일 때 “`해동된 배아 수`” ≠ 0 인 행 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_embryo_outliers(train_df):\n",
    "    \"\"\"\n",
    "    배아 관련 이상치 데이터를 제거하는 함수.\n",
    "    \n",
    "    - 특정 시술 유형이 IVF이면서, 특정 시술 유형에 ICSI가 포함되지 않는데 미세 관련 변수가 0이 아닌 행 삭제\n",
    "    - 특정 시술 유형이 IVF이면서, 이식된 배아 수가 0인데 배아 이식 경과일 > 0 인 행 삭제\n",
    "    - 특정 시술 유형이 IVF이면서, 동결 배아 사용 여부가 0인데 해동된 배아 수 > 0인 행 삭제\n",
    "    - 특정 시술 유형이 IVF이면서, 동결 배아 사용 여부가 0 & 배아 해동 경과일이 결측인데 해동된 배아 수 ≠ 0 인 행 삭제\n",
    "    \"\"\"\n",
    "\n",
    "    # 삭제 전 데이터 크기 저장\n",
    "    train_size_before = len(train_df)\n",
    "\n",
    "    # 1. 특정 시술 유형이 IVF이면서, 특정 시술 유형에 ICSI가 없는데 \"미세\" 포함 변수 값이 0이 아닌 행 삭제\n",
    "    micro_columns = [col for col in train_df.columns if \"미세\" in col]  # \"미세\" 포함된 컬럼 찾기\n",
    "    condition = (train_df[\"시술 유형\"] == \"IVF\") & (~train_df[\"특정 시술 유형\"].str.contains(\"ICSI\", na=False)) & (train_df[micro_columns].sum(axis=1) > 0)\n",
    "    train_df = train_df[~condition]\n",
    "\n",
    "    # 2. 특정 시술 유형이 IVF이면서, 이식된 배아 수 == 0인데 배아 이식 경과일 > 0 인 행 삭제\n",
    "    condition = (train_df[\"시술 유형\"] == \"IVF\") & (train_df[\"이식된 배아 수\"] == 0) & (train_df[\"배아 이식 경과일\"] > 0)\n",
    "    train_df = train_df[~condition]\n",
    "\n",
    "    # 3. 특정 시술 유형이 IVF이면서, 동결 배아 사용 여부 == 0인데 해동된 배아 수 > 0 인 행 삭제\n",
    "    condition = (train_df[\"시술 유형\"] == \"IVF\") & (train_df[\"동결 배아 사용 여부\"] == 0) & (train_df[\"해동된 배아 수\"] > 0)\n",
    "    train_df = train_df[~condition]\n",
    "\n",
    "    # 4. 특정 시술 유형이 IVF이면서, 동결 배아 사용 여부 == 0 & 배아 해동 경과일이 결측인데 해동된 배아 수 ≠ 0 인 행 삭제\n",
    "    condition = (train_df[\"시술 유형\"] == \"IVF\") & (train_df[\"동결 배아 사용 여부\"] == 0) & (train_df[\"배아 해동 경과일\"].isna()) & (train_df[\"해동된 배아 수\"] != 0)\n",
    "    train_df = train_df[~condition]\n",
    "\n",
    "    # 삭제 후 데이터 크기 저장\n",
    "    train_size_after = len(train_df)\n",
    "\n",
    "    # 삭제된 행 수 계산\n",
    "    train_removed = train_size_before - train_size_after\n",
    "    # 삭제된 행 수 출력\n",
    "    print(f\"Train 데이터에서 삭제된 행 수: {train_removed}\")\n",
    "\n",
    "    return train_df\n",
    "\n",
    "train = remove_embryo_outliers(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. '`동결 배아 사용 여부`' == 0, '`해동된 배아 수`' == 0, '`배아 해동 경과일`' != 0 인 행들을 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_embryo_rows(train_df):\n",
    "    \"\"\"\n",
    "    특정 시술 유형이 IVF인 경우,\n",
    "    '동결 배아 사용 여부' == 0, '해동된 배아 수' == 0, '배아 해동 경과일'이 NaN이 아닌 경우의 행을 삭제하는 함수.\n",
    "    \"\"\"\n",
    "\n",
    "    # 삭제 전 데이터 크기 저장\n",
    "    train_size_before = len(train_df)\n",
    "\n",
    "    # 특정 시술 유형이 IVF인 경우만 적용\n",
    "    condition = (train_df['시술 유형'] == \"IVF\") & \\\n",
    "                (train_df['동결 배아 사용 여부'] == 0) & \\\n",
    "                (train_df['해동된 배아 수'] == 0) & \\\n",
    "                (pd.notna(train_df['배아 해동 경과일']))  # NaN이 아닌 경우 필터링\n",
    "    train_df = train_df[~condition]  # 조건에 맞는 행 삭제\n",
    "\n",
    "    # 삭제된 행 수 계산\n",
    "    train_removed = train_size_before - len(train_df)\n",
    "\n",
    "    # 삭제된 행 수 출력\n",
    "    print(f\"Train 데이터에서 삭제된 행 수: {train_removed}\")\n",
    "\n",
    "    return train_df\n",
    "\n",
    "# 함수 사용 예시\n",
    "train = remove_invalid_embryo_rows(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. `특정 시술 유형` 컬럼 결측인 행 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_procedure(train_df):\n",
    "    \"\"\" \n",
    "    '특정 시술 유형' 컬럼이 존재할 경우, 결측(NaN)인 행을 제거하고, 제거된 행 수를 출력하는 함수.\n",
    "    컬럼이 없으면 경고 메시지를 출력하고 원래 데이터프레임을 반환.\n",
    "    \"\"\"\n",
    "\n",
    "    # 특정 시술 유형 컬럼 존재 여부 확인\n",
    "    if '특정 시술 유형' not in train_df.columns:\n",
    "        print(\"⚠️ '특정 시술 유형' 컬럼이 데이터프레임에 존재하지 않습니다. 원본 데이터를 반환합니다.\")\n",
    "        return train_df\n",
    "\n",
    "    # 원본 데이터 크기 저장\n",
    "    train_size_before = len(train_df)\n",
    "\n",
    "    # '특정 시술 유형'이 NaN인 행 제거\n",
    "    train_df = train_df.dropna(subset=['특정 시술 유형'])\n",
    "\n",
    "    # 제거된 행 수 계산\n",
    "    train_removed = train_size_before - len(train_df)\n",
    "\n",
    "    # 제거된 행 수 출력\n",
    "    print(f\"Train 데이터에서 제거된 행 수: {train_removed}\")\n",
    "\n",
    "    return train_df\n",
    "train = remove_outliers_procedure(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. 결측 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. IVF 시술을 받은 환자의 경우 결측처리\n",
    "- IVF 시술을 받은 환자의 경우 아래와 같이 결측 처리\n",
    "- '`이식된 배아 수`'가 0이면 '`배아 이식 경과일`'의 결측값을 0으로 채움.\n",
    "- '`동결 배아 사용 여부`'가 0이면 '`배아 해동 경과일`'의 결측값을 0으로 채움."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_embryo_days(train_df, test_df):\n",
    "    \"\"\"\n",
    "    IVF 시술을 받은 환자의 경우,\n",
    "    - '이식된 배아 수'가 0이면 '배아 이식 경과일'의 결측값을 0으로 채움.\n",
    "    - '동결 배아 사용 여부'가 0이면 '배아 해동 경과일'의 결측값을 0으로 채움.\n",
    "    \"\"\"\n",
    "\n",
    "    def fill_embryo_days(df):\n",
    "        # IVF 시술을 받은 경우만 적용\n",
    "        ivf_mask = df['시술 유형'] == \"IVF\"\n",
    "\n",
    "        # '이식된 배아 수' == 0 → '배아 이식 경과일' NaN을 0으로 채우기\n",
    "        condition_1 = ivf_mask & (df['이식된 배아 수'] == 0) & (df['배아 이식 경과일'].isna())\n",
    "        df.loc[condition_1, '배아 이식 경과일'] = 0\n",
    "\n",
    "        # '동결 배아 사용 여부' == 0 → '배아 해동 경과일' NaN을 0으로 채우기\n",
    "        condition_2 = ivf_mask & (df['동결 배아 사용 여부'] == 0) & (df['배아 해동 경과일'].isna())\n",
    "        df.loc[condition_2, '배아 해동 경과일'] = 0\n",
    "\n",
    "        # 변경된 데이터 개수 출력\n",
    "        print(f\"[데이터셋: {df.name}] '배아 이식 경과일' 채운 개수: {condition_1.sum()}\")\n",
    "        print(f\"[데이터셋: {df.name}] '배아 해동 경과일' 채운 개수: {condition_2.sum()}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    # DataFrame 이름 추가 (출력 시 구분하기 위해)\n",
    "    train_df.name = \"Train\"\n",
    "    test_df.name = \"Test\"\n",
    "\n",
    "    train_df = fill_embryo_days(train_df)\n",
    "    test_df = fill_embryo_days(test_df)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "#   함수 실행\n",
    "train, test = fill_missing_embryo_days(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. \"`특정 시술 유형`' 컬럼 결측 처리\n",
    "- \"Unknown\" => nan으로 변경\n",
    "- Unknown이 : 으로 묶인 경우 => Unknown 칼럼 생성\n",
    "- \"FER\", \"GIFT\" 는Unknown으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_treatment_types(train_df, test_df):\n",
    "    # 데이터 복사\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    train_df[\"특정 시술 유형\"] = train_df['특정 시술 유형'].replace(\"FER\", \"Unknown\").replace(\"GIFT\", \"Unknown\")\n",
    "    test_df[\"특정 시술 유형\"] = test_df['특정 시술 유형'].replace(\"FER\", \"Unknown\").replace(\"GIFT\", \"Unknown\")\n",
    "\n",
    "    # 특정 시술 유형에서 고유한 시술 종류 추출\n",
    "    all_treatments = set()\n",
    "\n",
    "    for df in [train_df, test_df]:\n",
    "        if '특정 시술 유형' in df.columns:\n",
    "            df['특정 시술 유형'] = df['특정 시술 유형'].astype(str).fillna(\"Unknown\")  # 결측값 방지\n",
    "            df['특정 시술 유형'].str.replace(\" \", \"\").str.split(\":|/\").apply(all_treatments.update)\n",
    "    \n",
    "    # 'nan' 값이 존재하면 제거\n",
    "    all_treatments.discard('nan')\n",
    "    \n",
    "    # 새로운 시술 유형 컬럼 생성 (초기값 0)\n",
    "    for treatment in all_treatments:\n",
    "        train_df[treatment] = 0\n",
    "        test_df[treatment] = 0\n",
    "\n",
    "    # 해당 시술이 포함된 경우 1로 설정\n",
    "    for df in [train_df, test_df]:\n",
    "        if '특정 시술 유형' in df.columns:\n",
    "            for treatment in all_treatments:\n",
    "                df[treatment] = df['특정 시술 유형'].str.replace(\" \", \"\").apply(lambda x: 1 if treatment in x else 0)\n",
    "\n",
    "    # 기존 '특정 시술 유형' 컬럼 삭제\n",
    "    train_df.drop(columns=['특정 시술 유형'], inplace=True, errors='ignore')\n",
    "    test_df.drop(columns=['특정 시술 유형'], inplace=True, errors='ignore')\n",
    "\n",
    "    # 추가된 칼럼 목록 출력\n",
    "    print(f\"추가된 시술 유형 칼럼들: {list(all_treatments)}\")\n",
    "    return train_df, test_df\n",
    "\n",
    "# 사용 예시\n",
    "train, test = encode_treatment_types(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Ordinal Encoding\n",
    "- 횟수 변수들 ordinal encoding\n",
    "- 총 ~ 횟수는 IVF ~ 횟수 + DI ~ 횟수로 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinal_encoding(train_df, test_df):\n",
    "    \"\"\"\n",
    "    나이 관련 변수를 순서형 숫자로 변환하고, 빈도 관련 변수를 정수 변환하는 함수.\n",
    "    \"\"\"\n",
    "\n",
    "    # 데이터 복사\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "\n",
    "\n",
    "    # 빈도 관련 컬럼 변환 (존재하는 컬럼만 처리)\n",
    "    freq_columns = ['총 시술 횟수', '클리닉 내 총 시술 횟수', 'IVF 시술 횟수', 'DI 시술 횟수', \n",
    "                    '총 임신 횟수', 'IVF 임신 횟수', 'DI 임신 횟수', '총 출산 횟수', 'IVF 출산 횟수', 'DI 출산 횟수']\n",
    "\n",
    "    for col in freq_columns:\n",
    "        if col in train_df.columns:\n",
    "            train_df[col] = train_df[col].apply(lambda x: int(str(x)[:1]) if pd.notna(x) and str(x)[0].isdigit() else x)\n",
    "            test_df[col] = test_df[col].apply(lambda x: int(str(x)[:1]) if pd.notna(x) and str(x)[0].isdigit() else x)\n",
    "    \n",
    "    train_df['총 시술 횟수'] = train_df['IVF 시술 횟수'] + train_df['DI 시술 횟수']\n",
    "    test_df['총 시술 횟수'] = test_df['IVF 시술 횟수'] + test_df['DI 시술 횟수']\n",
    "    \n",
    "    train_df['총 임신 횟수'] = train_df['IVF 임신 횟수'] + train_df['DI 임신 횟수']\n",
    "    test_df['총 임신 횟수'] = test_df['IVF 임신 횟수'] + test_df['DI 임신 횟수']\n",
    "    \n",
    "    train_df['총 출산 횟수'] = train_df['IVF 출산 횟수'] + train_df['DI 출산 횟수']\n",
    "    test_df['총 출산 횟수'] = test_df['IVF 출산 횟수'] + test_df['DI 출산 횟수']\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "# 사용 예시\n",
    "train, test = ordinal_encoding(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. 분산 0인 칼람 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_value_columns(train_df, test_df, fill_value=999):\n",
    "    \"\"\"\n",
    "    NaN을 임시 값으로 채운 후, 유일한 값이 하나뿐인 컬럼을 삭제하는 함수\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # NaN을 임시 값으로 채운 후, 유일한 값이 하나뿐인 컬럼 찾기\n",
    "    single_value_cols_train = train_df.fillna(fill_value).nunique() == 1\n",
    "\n",
    "    # 제거할 컬럼 리스트\n",
    "    cols_to_drop = train_df.columns[single_value_cols_train].tolist()\n",
    "\n",
    "    # 데이터에서 해당 컬럼 삭제\n",
    "    train_df = train_df.drop(columns=cols_to_drop)\n",
    "    test_df = test_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # 삭제된 컬럼 출력\n",
    "    print(f\"삭제된 단일 값 컬럼: {cols_to_drop}\")\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "train, test = remove_single_value_columns(train, test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. 파생 변수\n",
    "아래 파생 변수 추가\n",
    "- 시술 실패 관련\n",
    "    - `총 시술 실패 횟수` = 총 시술 횟수 - 총 임신 횟수\n",
    "    - `IVF 시술 실패 횟수` = IVF 시술 횟수 - IVF 임신 횟수\n",
    "    - `DI 시술 실패 횟수` = DI 시술 횟수 - DI 임신 횟수\n",
    "\n",
    "- 출산 실패 관련\n",
    "    - `총 출산 실패 횟수` = 총 임신 횟수 - 총 출산 횟수\n",
    "    - `IVF 출산 실패 횟수` = IVF 임신 횟수 - IVF 출산 횟수\n",
    "    - `DI 출산 실패 횟수` = DI 임신 횟수 - DI 출산 횟수\n",
    "\n",
    "- 시술 성공률 관련\n",
    "    - `총 임신 성공률` = 총 임신 횟수 / 총 시술 횟수\n",
    "    - `IVF 임신 성공률` = IVF 임신 횟수 / IVF 시술 횟수\n",
    "    - `DI 임신 성공률` = DI 임신 횟수 / DI 시술 횟수\n",
    "\n",
    "- 출산 성공률 관련\n",
    "    - `총 출산 성공률` = 총 출산 횟수 / 총 임신 횟수\n",
    "    - `IVF 출산 성공률` = IVF 출산 횟수 / IVF 임신 횟수\n",
    "    - `DI 출산 성공률` = DI 출산 횟수 / DI 임신 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_failure_counts(train_df, test_df):\n",
    "    \"\"\"\n",
    "    시술 실패 횟수 및 출산 실패 횟수를 계산하여 추가하는 함수.\n",
    "    \"\"\"\n",
    "\n",
    "    # 시술 실패 횟수 계산\n",
    "    train_df[\"총 시술 실패 횟수\"] = train_df[\"총 시술 횟수\"] - train_df[\"총 임신 횟수\"]\n",
    "    train_df[\"IVF 시술 실패 횟수\"] = train_df[\"IVF 시술 횟수\"] - train_df[\"IVF 임신 횟수\"]\n",
    "    train_df[\"DI 시술 실패 횟수\"] = train_df[\"DI 시술 횟수\"] - train_df[\"DI 임신 횟수\"]\n",
    "\n",
    "    test_df[\"총 시술 실패 횟수\"] = test_df[\"총 시술 횟수\"] - test_df[\"총 임신 횟수\"]\n",
    "    test_df[\"IVF 시술 실패 횟수\"] = test_df[\"IVF 시술 횟수\"] - test_df[\"IVF 임신 횟수\"]\n",
    "    test_df[\"DI 시술 실패 횟수\"] = test_df[\"DI 시술 횟수\"] - test_df[\"DI 임신 횟수\"]\n",
    "\n",
    "    # 출산 실패 횟수 계산\n",
    "    train_df[\"총 출산 실패 횟수\"] = train_df[\"총 임신 횟수\"] - train_df[\"총 출산 횟수\"]\n",
    "    train_df[\"IVF 출산 실패 횟수\"] = train_df[\"IVF 임신 횟수\"] - train_df[\"IVF 출산 횟수\"]\n",
    "    train_df[\"DI 출산 실패 횟수\"] = train_df[\"DI 임신 횟수\"] - train_df[\"DI 출산 횟수\"]\n",
    "\n",
    "    test_df[\"총 출산 실패 횟수\"] = test_df[\"총 임신 횟수\"] - test_df[\"총 출산 횟수\"]\n",
    "    test_df[\"IVF 출산 실패 횟수\"] = test_df[\"IVF 임신 횟수\"] - test_df[\"IVF 출산 횟수\"]\n",
    "    test_df[\"DI 출산 실패 횟수\"] = test_df[\"DI 임신 횟수\"] - test_df[\"DI 출산 횟수\"]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "train, test = add_failure_counts(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_success_rate_and_treatment_experience(train_df, test_df):\n",
    "    \"\"\"\n",
    "    성공률 및 시술 경험 여부 변수를 추가하는 함수.\n",
    "    \"\"\"\n",
    "\n",
    "    #   성공률 계산을 위한 함수 (0으로 나누는 경우 방지)\n",
    "    def compute_success_rate(numerator, denominator):\n",
    "        return numerator / (denominator + 1)  #   시술 횟수가 0인 경우 방지\n",
    "\n",
    "    #   성공률 변수 추가\n",
    "    for df in [train_df, test_df]:\n",
    "        df[\"총 임신 성공률\"] = compute_success_rate(df[\"총 임신 횟수\"], df[\"총 시술 횟수\"])\n",
    "        df[\"IVF 임신 성공률\"] = compute_success_rate(df[\"IVF 임신 횟수\"], df[\"IVF 시술 횟수\"])\n",
    "        df[\"DI 임신 성공률\"] = compute_success_rate(df[\"DI 임신 횟수\"], df[\"DI 시술 횟수\"])\n",
    "        df[\"총 출산 성공률\"] = compute_success_rate(df[\"총 출산 횟수\"], df[\"총 임신 횟수\"])\n",
    "        df[\"IVF 출산 성공률\"] = compute_success_rate(df[\"IVF 출산 횟수\"], df[\"IVF 임신 횟수\"])\n",
    "        df[\"DI 출산 성공률\"] = compute_success_rate(df[\"DI 출산 횟수\"], df[\"DI 임신 횟수\"])\n",
    "\n",
    "        #   시술 경험 여부 변수 추가\n",
    "        df[\"전체 시술 경험 여부\"] = (df[\"총 시술 횟수\"] > 0).astype(int)   # 1회 이상 시술한 경우 1, 아니면 0\n",
    "        df[\"IVF 시술 경험 여부\"] = (df[\"IVF 시술 횟수\"] > 0).astype(int)   # IVF 시술 경험 여부\n",
    "        df[\"DI 시술 경험 여부\"] = (df[\"DI 시술 횟수\"] > 0).astype(int)     # DI 시술 경험 여부\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "#   함수 실행\n",
    "train, test = add_success_rate_and_treatment_experience(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  `자연 수정 배아 비율` \n",
    "\n",
    "    IVF 시술을 받은 환자의 경우,\n",
    "\n",
    "    - '자연 수정 배아 비율' = 1 - (미세 주입 배아 이식 수 / (이식된 배아 수 + 1)) 변수 추가\n",
    "    - DI 시술을 받은 환자의 경우, 해당 변수를 NaN으로 설정\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_natural_fertilization_rate(train_df, test_df):\n",
    "    \"\"\"\n",
    "    IVF 시술을 받은 환자의 경우,\n",
    "    - '자연 수정 배아 비율' = 1 - (미세 주입 배아 이식 수 / (이식된 배아 수 + 1)) 변수 추가\n",
    "    - DI 시술을 받은 환자의 경우, 해당 변수를 NaN으로 설정\n",
    "    \n",
    "    train_df와 test_df 모두 동일한 로직을 적용함.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_natural_fertilization(df):\n",
    "        #   IVF 시술을 받은 경우만 계산\n",
    "        ivf_mask = df['시술 유형'] == \"IVF\"\n",
    "\n",
    "        #   자연 수정 배아 비율 계산 (IVF만 적용)\n",
    "        df.loc[ivf_mask, '자연 수정 배아 비율'] = 1 - (df['미세주입 배아 이식 수'] / (df['이식된 배아 수'] + 1))\n",
    "\n",
    "        #   DI 시술을 받은 경우 NaN 설정\n",
    "        di_mask = df['시술 유형'] == \"DI\"\n",
    "        df.loc[di_mask, '자연 수정 배아 비율'] = np.nan\n",
    "\n",
    "        #   변경된 데이터 개수 출력\n",
    "        print(f\" [데이터셋: {df.name}] IVF 자연 수정 배아 비율 추가 개수: {ivf_mask.sum()}\")\n",
    "        print(f\" [데이터셋: {df.name}] DI 자연 수정 배아 비율 NaN 처리 개수: {di_mask.sum()}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    #   DataFrame 이름 추가 (출력 시 구분하기 위해)\n",
    "    train_df.name = \"Train\"\n",
    "    test_df.name = \"Test\"\n",
    "\n",
    "    train_df = compute_natural_fertilization(train_df)\n",
    "    test_df = compute_natural_fertilization(test_df)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "#   함수 실행\n",
    "train, test = add_natural_fertilization_rate(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. 추가 결측 처리\n",
    "`배아 이식 경과일` 컬럼에 IterativeImputer 적용하여 추가적으로 결측 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mice_embryo_days(train_df, test_df):\n",
    "    # IVF 시술 유형 데이터 필터링\n",
    "    train_ivf = train_df[train_df[\"시술 유형\"] == \"IVF\"].copy()\n",
    "    test_ivf = test_df[test_df[\"시술 유형\"] == \"IVF\"].copy()\n",
    "    \n",
    "    # 사용할 변수 목록 (타겟 + 상관관계 높은 변수)\n",
    "    features = [\n",
    "        \"배아 이식 경과일\", \"배란 자극 여부\", \"총 시술 횟수\", \"IVF 시술 횟수\",\n",
    "        \"총 생성 배아 수\", \"미세주입된 난자 수\", \"미세주입에서 생성된 배아 수\",\n",
    "        \"저장된 배아 수\", \"해동된 배아 수\", \"수집된 신선 난자 수\", \"혼합된 난자 수\",\n",
    "        \"파트너 정자와 혼합된 난자 수\", \"동결 배아 사용 여부\", \"신선 배아 사용 여부\",\n",
    "        \"난자 채취 경과일\"\n",
    "    ]\n",
    "\n",
    "    # MICE 적용할 데이터셋 추출\n",
    "    train_mice = train_ivf[features]\n",
    "    test_mice = test_ivf[features]\n",
    "\n",
    "    # MICE 모델 학습 (train 데이터만 사용)\n",
    "    # imputer = IterativeImputer(estimator=RandomForestRegressor(n_estimators=100), max_iter=10, random_state=42)\n",
    "    mice_imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "    train_imputed = mice_imputer.fit_transform(train_mice)\n",
    "\n",
    "    # 훈련된 MICE 모델로 test 데이터 결측 보강\n",
    "    test_imputed = mice_imputer.transform(test_mice)\n",
    "\n",
    "    # 보강된 값 적용 (DataFrame으로 변환)\n",
    "    train_ivf_imputed = pd.DataFrame(train_imputed, columns=features, index=train_ivf.index)\n",
    "    test_ivf_imputed = pd.DataFrame(test_imputed, columns=features, index=test_ivf.index)\n",
    "\n",
    "    # 0보다 작은 값은 0으로 변경, 7 이상인 값은 7로 제한 후, 정수로 반올림\n",
    "    train_ivf_imputed[\"배아 이식 경과일\"] = train_ivf_imputed[\"배아 이식 경과일\"].clip(lower=0, upper=10)\n",
    "    test_ivf_imputed[\"배아 이식 경과일\"] = test_ivf_imputed[\"배아 이식 경과일\"].clip(lower=0, upper=10)\n",
    "\n",
    "    # 원본 데이터에 보강된 값 반영 (DI 시술 데이터는 변경 없음)\n",
    "    train_df.loc[train_ivf.index, \"배아 이식 경과일\"] = train_ivf_imputed[\"배아 이식 경과일\"]\n",
    "    test_df.loc[test_ivf.index, \"배아 이식 경과일\"] = test_ivf_imputed[\"배아 이식 경과일\"]\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train, test = mice_embryo_days(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8. 연속형 변수 범주형 변환\n",
    "- `해동 난자 수 컬럼` 을 구간을 나누어 범주형으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numeric_to_categorical(train_df, test_df, bin_cols):\n",
    "    \"\"\"\n",
    "    특정 칼럼들의 값을 구간을 나눠 범주형으로 변환하는 함수.\n",
    "    \"\"\"\n",
    "    #   범주화할 구간 설정 (0은 그대로 두고, 이후 5단위 구간 생성)\n",
    "    bins = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 25, 30, 40, 50, 100]  # 원하는 범위 추가 가능\n",
    "    labels = [f\"{bins[i]+1}-{bins[i+1]}\" for i in range(len(bins)-1)]  # \"1-5\", \"6-10\" 같은 라벨 자동 생성\n",
    "\n",
    "    for df in [train_df, test_df]:\n",
    "        for col in bin_cols:\n",
    "            df[col] = pd.cut(df[col], bins=[-1] + bins, labels=[\"0\"] + labels).astype(str)  #   -1을 추가해 0 유지\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "#   함수 실행 예시\n",
    "bin_cols = [\"해동 난자 수\"]  # 변환할 변수 리스트\n",
    "train, test = convert_numeric_to_categorical(train, test, bin_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing data 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(dir + 'train_Preprocessed.csv', encoding = 'utf-8')\n",
    "test.to_csv(dir + 'test_Preprocessed.csv', encoding = 'utf-8')\n",
    "print(f'train_Preprocessed.csv, test_Preprocessed.csv {dir}에 저장완료')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정된 컬럼명으로 X, y 분리\n",
    "X = train.drop(columns=['임신 성공 여부'])\n",
    "y = train['임신 성공 여부']\n",
    "\n",
    "\n",
    "# X에서 object 타입(범주형)인 컬럼 찾기\n",
    "cat_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "X[cat_columns] = X[cat_columns].astype(str)\n",
    "test[cat_columns] = test[cat_columns].astype(str)\n",
    "\n",
    "# 결과 확인\n",
    "print(\" 범주형 변수 목록:\", cat_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Gridsearch\n",
    "- params\n",
    "    - `N_JOBS` : 모델 훈련에 사용할 CPU 코어 수\n",
    "    - `cv_seed` : cross-validation random seed\n",
    "    - `n_cv` : cross-validation seed 수 \n",
    "    - `param_grid` : grid setting\n",
    "    - `models` : 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 병렬 처리 설정\n",
    "N_JOBS = 1  # CPU 코어 개수 (변경 가능)\n",
    "cv_seed = 5534  # n_cv fold cv 의 시드\n",
    "n_cv = 3\n",
    "n_iter = 1 # Bayes Search 횟수\n",
    "\n",
    "param_grid = {\n",
    "    \"CatBoost\": {\n",
    "        \"border_count\" : [64],\n",
    "        \"depth\": [7], \n",
    "        \"learning_rate\": [0.032250131659519184], \n",
    "        \"iterations\": [920],\n",
    "        \"l2_leaf_reg\" : [145],\n",
    "        \"scale_pos_weight\" : [3.510614653412628]    \n",
    "    }\n",
    "}\n",
    "############################## 최적값 ##############################\n",
    "\n",
    "# 모델 정의\n",
    "models = {\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "                verbose=0, \n",
    "                random_state=42,\n",
    "                task_type=\"GPU\",\n",
    "                cat_features=cat_columns,\n",
    "\n",
    "                )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CatBoost 전용 Pool 객체 생성\n",
    "train_pool = Pool(data=X, label=y, cat_features=cat_columns)\n",
    "\n",
    "# AUC 점수를 소수점 6자리까지 출력하는 custom scorer\n",
    "def custom_auc_scorer(y_true, y_pred):\n",
    "    auc_score = roc_auc_score(y_true, y_pred)\n",
    "    rounded_score = round(auc_score, 6)  # 소수점 6자리 반올림\n",
    "    print(f\" Fold AUC: {rounded_score}\")  # Fold마다 출력\n",
    "    return auc_score\n",
    "\n",
    "# 결과 저장\n",
    "best_models = {}\n",
    "cv_results = {}\n",
    "auc_results = {}\n",
    "\n",
    "start_time = time.time()  \n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n {model_name} 모델 GridSearch 시작...\")\n",
    "    model_start = time.time()  \n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_cv, shuffle=True, random_state=cv_seed)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "    model, \n",
    "    param_grid[model_name], \n",
    "    scoring=make_scorer(custom_auc_scorer, response_method=\"predict_proba\"),  \n",
    "    cv=cv, \n",
    "    n_jobs=N_JOBS,  #  병렬 처리\n",
    "    verbose=n_cv\n",
    "    )\n",
    "\n",
    "    \n",
    "    #   `cat_features`를 GridSearch에도 적용 (astype('category') 변환)\n",
    "    X_catboost = X.copy()\n",
    "    X_catboost[cat_columns] = X_catboost[cat_columns].astype('category')  #  범주형 변수 변환\n",
    "    \n",
    "    #   GridSearchCV의 fit()에서 cat_features 명시적으로 전달\n",
    "    grid_search.fit(X_catboost, y)  \n",
    "\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    print(f\"  {model_name} 최적 파라미터: {grid_search.best_params_}\")\n",
    "\n",
    "    print(f\" {model_name} {n_cv}-Fold Cross Validation 시작...\")\n",
    "    train_auc_scores = []\n",
    "    val_auc_scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in tqdm(enumerate(cv.split(X, y), 1), total=n_cv, desc=f\" {model_name} CV Progress\"):\n",
    "        X_t, X_v = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_t, y_v = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        #   학습 & 검증용 Pool 생성 (범주형 변수 설정)\n",
    "        train_fold_pool = Pool(data=X_t, label=y_t, cat_features=cat_columns)\n",
    "        val_fold_pool = Pool(data=X_v, label=y_v, cat_features=cat_columns)\n",
    "\n",
    "        model = grid_search.best_estimator_\n",
    "        model.fit(train_fold_pool)  #  `Pool`을 사용하여 학습\n",
    "\n",
    "        #   Train AUC 계산\n",
    "        y_train_pred = model.predict_proba(train_fold_pool)[:, 1]\n",
    "        train_auc = roc_auc_score(y_t, y_train_pred)\n",
    "        train_auc_scores.append(round(train_auc, 6))\n",
    "\n",
    "        #   Validation AUC 계산\n",
    "        y_val_pred = model.predict_proba(val_fold_pool)[:, 1]\n",
    "        val_auc = roc_auc_score(y_v, y_val_pred)\n",
    "        val_auc_scores.append(round(val_auc, 6))\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"\\n {model_name} {n_cv}-Fold Train AUC Scores = {train_auc_scores}\")\n",
    "    print(f\" {model_name} {n_cv}-Fold Validation AUC Scores = {val_auc_scores}\")\n",
    "\n",
    "    mean_train_auc = np.mean(train_auc_scores)\n",
    "    std_train_auc = np.std(train_auc_scores)\n",
    "\n",
    "    mean_val_auc = np.mean(val_auc_scores)\n",
    "    std_val_auc = np.std(val_auc_scores)\n",
    "\n",
    "    cv_results[model_name] = {\"Train AUC\": (mean_train_auc, std_train_auc), \"Validation AUC\": (mean_val_auc, std_val_auc)}\n",
    "    auc_results[model_name] = {\"Train\": train_auc_scores, \"Validation\": val_auc_scores}\n",
    "\n",
    "    print(f\" {model_name} Train AUC: 평균={mean_train_auc:.6f}, 표준편차={std_train_auc:.6f}\")\n",
    "    print(f\" {model_name} Validation AUC: 평균={mean_val_auc:.6f}, 표준편차={std_val_auc:.6f}\")\n",
    "\n",
    "    model_end = time.time()\n",
    "    print(f\" {model_name} 학습 완료! 소요 시간: {model_end - model_start:.2f}초\\n\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n 전체 학습 완료! 총 소요 시간: {total_time:.2f}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n 최적 모델 및 AUC 결과 정리\")\n",
    "\n",
    "#   CV 설정 정보 출력\n",
    "print(f\"cv random_seed: {cv_seed}, n_folds: {n_cv}\")\n",
    "\n",
    "#   CatBoost 최적 하이퍼파라미터 출력\n",
    "best_catboost_params = best_models[\"CatBoost\"].get_params()\n",
    "print(\"\\n Best CatBoost Hyperparameters:\")\n",
    "for key, value in best_catboost_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "#   Train & Validation AUC 결과 출력\n",
    "mean_train_auc, std_train_auc = cv_results[\"CatBoost\"][\"Train AUC\"]\n",
    "mean_val_auc, std_val_auc = cv_results[\"CatBoost\"][\"Validation AUC\"]\n",
    "\n",
    "print(f\"\\n {n_cv}-Fold Train AUC Scores: {auc_results['CatBoost']['Train']} (std: {std_train_auc:.6f})\")\n",
    "print(f\" {n_cv}-Fold Validation AUC Scores: {auc_results['CatBoost']['Validation']} (std: {std_val_auc:.6f})\")\n",
    "\n",
    "print(f\"\\n Final Train AUC: 평균={mean_train_auc:.6f}, 표준편차={std_train_auc:.6f}\")\n",
    "print(f\" Final Validation AUC: 평균={mean_val_auc:.6f}, 표준편차={std_val_auc:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7️⃣ 최적 하이퍼파라미터로 최종 모델 학습 (전체 데이터 사용)\n",
    "print(\"\\n 최적 하이퍼파라미터로 Final Model 학습 시작...\")\n",
    "\n",
    "#   최적 하이퍼파라미터 가져오기\n",
    "best_params = best_models[\"CatBoost\"].get_params()\n",
    "best_params.update({\"verbose\": 0, \"random_state\": 42, \"thread_count\": N_JOBS})  #   불필요한 verbose 제거\n",
    "print(f\"finalized params : {best_params}\")\n",
    "\n",
    "#   최종 학습 데이터 Pool 생성 (범주형 변수 포함)\n",
    "train_pool = Pool(data=X, label=y, cat_features=cat_columns)\n",
    "\n",
    "#   최종 모델 훈련 (Pool 사용)\n",
    "final_model = CatBoostClassifier(**best_params)\n",
    "final_model.fit(train_pool)\n",
    "\n",
    "print(\"  Final Model 학습 완료!\")\n",
    "# 0.743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  최종 모델 저장\n",
    "joblib.dump(final_model, dir_models + \"final_cat_model.pkl\")\n",
    "print(f\" 최적 CatBoost 모델이 `final_cat_model.pkl`로 저장되었습니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8️⃣ Test 데이터에 대해 predict_proba 수행 (Pool 사용)\n",
    "print(\"\\n Test 데이터 예측 (predict_proba) 시작...\")\n",
    "\n",
    "#   Test 데이터 Pool 생성\n",
    "X_test = test.copy()\n",
    "test_pool = Pool(data=X_test, cat_features=cat_columns)  #   Pool 생성\n",
    "\n",
    "#   예측 수행 (Pool 사용)\n",
    "test_preds = final_model.predict_proba(test_pool)[:, 1]  #   NumPy 배열 반환\n",
    "\n",
    "pd.DataFrame(test_preds).to_csv(dir_to_submit + 'test_pred_cat.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. train predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pool = Pool(data = X, cat_features= cat_columns)\n",
    "train_preds = final_model.predict_proba(t_pool)[:, 1]\n",
    "\n",
    "t_prob = pd.DataFrame(train_preds, train['임신 성공 여부'])\n",
    "\n",
    "# 제출할 파일명\n",
    "file_name = dir_to_submit + \"train_pred_cat.csv\"\n",
    "t_prob.to_csv(file_name, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. XgBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(dir + \"train_Preprocessed.csv\", encoding='utf-8', index_col = 0)\n",
    "test = pd.read_csv(dir + \"test_Preprocessed.csv\", encoding='utf-8', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. 수치형, 범주형 칼럼 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [ \"총 시술 횟수\", \"클리닉 내 총 시술 횟수\", \"IVF 시술 횟수\", \"DI 시술 횟수\",\n",
    "                   \"총 임신 횟수\", \"IVF 임신 횟수\", \"DI 임신 횟수\", \"총 출산 횟수\", \"IVF 출산 횟수\", \"DI 출산 횟수\", \"총 생성 배아 수\",\n",
    "                  \"미세주입에서 생성된 배아 수\", \"미세주입 배아 이식 수\", \"저장된 배아 수\", \"미세주입 후 저장된 배아 수\",\n",
    "                   \"저장된 신선 난자 수\",  \"기증자 정자와 혼합된 난자 수\",\n",
    "                   \"난자 혼합 경과일\", \"배아 이식 경과일\", \"배아 해동 경과일\", \"총 시술 실패 횟수\", \"IVF 시술 실패 횟수\", \"DI 시술 실패 횟수\", \"총 출산 실패 횟수\",\n",
    "                   \"IVF 출산 실패 횟수\", \"DI 출산 실패 횟수\", \n",
    "                   \"총 임신 성공률\", \"IVF 임신 성공률\", \"DI 임신 성공률\", \"총 출산 성공률\", \"IVF 출산 성공률\", \"DI 출산 성공률\",\n",
    "                   \"자연 수정 배아 비율\",\n",
    "                    # \"해동 난자 수\",    # 범주형으로 두는게 스코어 더 높음\n",
    "                    \"수집된 신선 난자 수\",\n",
    "                    \"혼합된 난자 수\",\n",
    "                    \"파트너 정자와 혼합된 난자 수\",\n",
    "                    \"이식된 배아 수\",   # 이건 수치형이 좋음.\n",
    "                    \"해동된 배아 수\",   # 이것도 수치형이 좋음.\n",
    "                    \"임신 시도 또는 마지막 임신 경과 연수\", # 이것도 수치형\n",
    "                    \"미세주입된 난자 수\",\n",
    "                   ]\n",
    "\n",
    "def convert_to_categorical(train_df, test_df, numeric_columns):\n",
    "    \"\"\"\n",
    "    특정 numeric_columns를 제외한 나머지 모든 컬럼을 범주형(Categorical)으로 변환하는 함수.\n",
    "    \"\"\"\n",
    "    \n",
    "    #   변환 대상 컬럼 찾기 (numeric_columns에 없는 컬럼들)\n",
    "    categorical_columns = [col for col in train_df.drop(columns = ['임신 성공 여부']).columns if col not in numeric_columns]\n",
    "\n",
    "    #   Categorical 변환\n",
    "    train_df[categorical_columns] = train_df[categorical_columns].astype(\"object\")\n",
    "    test_df[categorical_columns] = test_df[categorical_columns].astype(\"object\")\n",
    "\n",
    "    print(f\"  변환 완료! 범주형(object)으로 변환된 컬럼 개수: {len(categorical_columns)}개\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "#   함수 실행\n",
    "train, test = convert_to_categorical(train, test, numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정된 컬럼명으로 X, y 분리\n",
    "X = train.drop(columns=['임신 성공 여부'])\n",
    "y = train['임신 성공 여부']\n",
    "\n",
    "\n",
    "# X에서 object 타입(범주형)인 컬럼 찾기\n",
    "cat_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "X[cat_columns] = X[cat_columns].astype(str)\n",
    "test[cat_columns] = test[cat_columns].astype(str)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"  범주형 변수 목록:\", cat_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   데이터 타입 변환 (범주형 변수들을 category로 변환)\n",
    "X_xgb = X.copy()\n",
    "for col in X_xgb.select_dtypes(include=[\"object\"]).columns:\n",
    "    X_xgb[col] = X_xgb[col].astype(\"category\")  #   범주형 변수를 category 타입으로 변환\n",
    "\n",
    "#   Test 데이터 전처리 (범주형 변수 변환)\n",
    "X_test_no_target = test.copy()  # Test 데이터 복사 (타겟 없음)\n",
    "for col in X_test_no_target.select_dtypes(include=[\"object\"]).columns:\n",
    "    X_test_no_target[col] = X_test_no_target[col].astype(\"category\")  #   범주형 변수를 category 타입으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Stratified K-Fold 설정 (5-Fold CV)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#   AUC 점수를 소수점 6자리까지 출력하는 custom scorer\n",
    "def custom_auc_scorer(y_true, y_pred):\n",
    "    auc_score = roc_auc_score(y_true, y_pred)\n",
    "    rounded_score = round(auc_score, 6)  # 소수점 6자리 반올림\n",
    "    print(f\"  Fold AUC: {rounded_score}\")  # Fold마다 출력\n",
    "    return auc_score\n",
    "\n",
    "#   XGBoost 하이퍼파라미터 검색 공간\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.036549283154481964],\n",
    "    \"n_estimators\": [434],\n",
    "    \"max_depth\": [4],\n",
    "    \"min_child_weight\": [1],\n",
    "    \"gamma\": [4],\n",
    "    \"subsample\":[0.5879532027938977],\n",
    "    \"colsample_bytree\": [0.7904350112349561],\n",
    "    \"lambda\": [4],\n",
    "    \"alpha\": [10],\n",
    "    \"scale_pos_weight\": [6],\n",
    "    \"tree_method\": [\"hist\"]\n",
    "}\n",
    "\n",
    "#   XGBoost 모델 정의\n",
    "xgb_model = XGBClassifier(\n",
    "    enable_categorical=True,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    verbosity=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#   Cross Validation 횟수 및 랜덤 시드 설정\n",
    "n_cv = 3  # Cross Validation Fold 개수 설정 가능\n",
    "cv_seed = 5534  # 랜덤 시드 설정\n",
    "\n",
    "#   Stratified K-Fold 설정 (n_cv 사용)\n",
    "cv = StratifiedKFold(n_splits=n_cv, shuffle=True, random_state=cv_seed)\n",
    "\n",
    "#   AUC 점수를 소수점 6자리까지 출력하는 custom scorer\n",
    "def custom_auc_scorer(y_true, y_pred):\n",
    "    auc_score = roc_auc_score(y_true, y_pred)\n",
    "    rounded_score = round(auc_score, 6)  # 소수점 6자리 반올림\n",
    "    print(f\"  Fold AUC: {rounded_score}\")  # Fold마다 출력\n",
    "    return auc_score\n",
    "\n",
    "#   Bayesian Optimization 실행\n",
    "start_time = time.time()\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    xgb_model,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(custom_auc_scorer, response_method=\"predict_proba\"),\n",
    "    cv=cv,  #   Cross Validation 횟수 적용 (n_cv)\n",
    "    n_iter=1,  #   최대 30번 탐색\n",
    "    n_jobs=30,  #   병렬 처리\n",
    "    verbose=3,\n",
    "    random_state=cv_seed  #   랜덤 시드 설정\n",
    ")\n",
    "\n",
    "# 💡 하이퍼파라미터 튜닝 실행\n",
    "bayes_search.fit(X_xgb, y)\n",
    "\n",
    "#   최적 파라미터 출력 (OrderedDict 형태로 변환)\n",
    "best_params = OrderedDict(bayes_search.best_params_)\n",
    "print(f\"  XGBoost 최적 파라미터: {best_params}\")\n",
    "\n",
    "#   XGBoost n_cv-Fold Cross Validation 시작...\n",
    "train_auc_scores = []\n",
    "val_auc_scores = []\n",
    "\n",
    "cv_start = time.time()\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in tqdm(enumerate(cv.split(X, y), 1), total=n_cv, desc=\"🔄 XGBoost CV Progress\"):\n",
    "    X_t, X_v = X_xgb.iloc[train_idx], X_xgb.iloc[val_idx]\n",
    "    y_t, y_v = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    #   최적 파라미터 적용한 모델 생성\n",
    "    model = XGBClassifier(**best_params, enable_categorical=True, random_state=cv_seed)\n",
    "    \n",
    "    #   학습\n",
    "    model.fit(X_t, y_t)\n",
    "\n",
    "    #   Train AUC 계산\n",
    "    y_train_pred = model.predict_proba(X_t)[:, 1]\n",
    "    train_auc = roc_auc_score(y_t, y_train_pred)\n",
    "    train_auc_scores.append(round(train_auc, 6))\n",
    "\n",
    "    #   Validation AUC 계산\n",
    "    y_val_pred = model.predict_proba(X_v)[:, 1]\n",
    "    val_auc = roc_auc_score(y_v, y_val_pred)\n",
    "    val_auc_scores.append(round(val_auc, 6))\n",
    "\n",
    "cv_end = time.time()\n",
    "cv_duration = cv_end - cv_start\n",
    "\n",
    "#   XGBoost 결과 출력\n",
    "print(f\"\\n  XGBoost {n_cv}-Fold Train AUC Scores = {train_auc_scores}\")\n",
    "print(f\"  XGBoost {n_cv}-Fold Validation AUC Scores = {val_auc_scores}\")\n",
    "\n",
    "mean_train_auc = np.mean(train_auc_scores)\n",
    "std_train_auc = np.std(train_auc_scores)\n",
    "\n",
    "mean_val_auc = np.mean(val_auc_scores)\n",
    "std_val_auc = np.std(val_auc_scores)\n",
    "\n",
    "print(f\"  XGBoost Train AUC: 평균={mean_train_auc:.6f}, 표준편차={std_train_auc:.6f}\")\n",
    "print(f\"  XGBoost Validation AUC: 평균={mean_val_auc:.6f}, 표준편차={std_val_auc:.6f}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"  XGBoost 학습 완료! 소요 시간: {total_time:.2f}초\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   1️⃣ 최적의 하이퍼파라미터 로드\n",
    "best_params[\"objective\"] = \"binary:logistic\"  # 이진 분류 문제\n",
    "best_params[\"eval_metric\"] = \"auc\"  # AUC 기준으로 성능 평가\n",
    "\n",
    "#   2️⃣ 최적 하이퍼파라미터로 모델 생성\n",
    "final_xgb_model = XGBClassifier(\n",
    "    **best_params,\n",
    "    enable_categorical=True,  #   범주형 데이터 직접 처리\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "#   3️⃣ 전체 데이터로 최종 모델 학습 (Validation 없이)\n",
    "final_xgb_model.fit(X_xgb, y)\n",
    "\n",
    "#   4️⃣ 모델 저장 (Pickle 파일로 저장하여 재사용 가능)\n",
    "joblib.dump(final_xgb_model, dir_models + \"final_xgb_model.pkl\")\n",
    "print(\"  최종 모델이 `final_xgb_model.pkl`로 저장되었습니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1. train predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_models + \"final_xgb_model.pkl\", \"rb\") as f:\n",
    "    final_xgb_model = pickle.load(f)\n",
    "\n",
    "train_pred_xgb = final_xgb_model.predict_proba(X_xgb)[:,1]\n",
    "pd.DataFrame(train_pred_xgb).to_csv(dir_to_submit + 'train_pred_xgb.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2. test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   3️⃣ 모델 예측 (확률 예측)\n",
    "y_test_pred_proba = final_xgb_model.predict_proba(X_test_no_target)[:, 1]  # 클래스 1의 확률 예측\n",
    "pd.DataFrame(y_test_pred_proba).to_csv(dir_to_submit + 'test_pred_xgb.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Preprocessing\n",
    "\n",
    "- BERT는 따로 Preprocessing\n",
    "- 기존 전처리와 동일하나 이상치만 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(dir + \"train.csv\",encoding='utf-8', index_col = 0)\n",
    "test_df = pd.read_csv(dir + \"test.csv\",encoding='utf-8', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_embryo_outliers(train_df):\n",
    "    \"\"\"\n",
    "    배아 관련 이상치 데이터를 제거하는 함수.\n",
    "\n",
    "    - 특정 시술 유형이 IVF이면서, 특정 시술 유형에 ICSI가 포함되지 않는데 미세 관련 변수가 0이 아닌 행 삭제\n",
    "    - 특정 시술 유형이 IVF이면서, 이식된 배아 수가 0인데 배아 이식 경과일 > 0 인 행 삭제\n",
    "    - 특정 시술 유형이 IVF이면서, 동결 배아 사용 여부가 0인데 해동된 배아 수 > 0인 행 삭제\n",
    "    - 특정 시술 유형이 IVF이면서, 동결 배아 사용 여부가 0 & 배아 해동 경과일이 결측인데 해동된 배아 수 ≠ 0 인 행 삭제\n",
    "    \"\"\"\n",
    "\n",
    "    # 삭제 전 데이터 크기 저장\n",
    "    train_size_before = len(train_df)\n",
    "\n",
    "    ###   1. 특정 시술 유형이 IVF이면서, 특정 시술 유형에 ICSI가 없는데 \"미세\" 포함 변수 값이 0이 아닌 행 삭제\n",
    "    micro_columns = [col for col in train_df.columns if \"미세\" in col]  # \"미세\" 포함된 컬럼 찾기\n",
    "    condition = (train_df[\"시술 유형\"] == \"IVF\") & (~train_df[\"특정 시술 유형\"].str.contains(\"ICSI\", na=False)) & (train_df[micro_columns].sum(axis=1) > 0)\n",
    "    train_df = train_df[~condition]\n",
    "\n",
    "    ###   2. 특정 시술 유형이 IVF이면서, 이식된 배아 수 == 0인데 배아 이식 경과일 > 0 인 행 삭제\n",
    "    condition = (train_df[\"시술 유형\"] == \"IVF\") & (train_df[\"이식된 배아 수\"] == 0) & (train_df[\"배아 이식 경과일\"] > 0)\n",
    "    train_df = train_df[~condition]\n",
    "\n",
    "    ###   3. 특정 시술 유형이 IVF이면서, 동결 배아 사용 여부 == 0인데 해동된 배아 수 > 0 인 행 삭제\n",
    "    condition = (train_df[\"시술 유형\"] == \"IVF\") & (train_df[\"동결 배아 사용 여부\"] == 0) & (train_df[\"해동된 배아 수\"] > 0)\n",
    "    train_df = train_df[~condition]\n",
    "\n",
    "    ###   4. 특정 시술 유형이 IVF이면서, 동결 배아 사용 여부 == 0 & 배아 해동 경과일이 결측인데 해동된 배아 수 ≠ 0 인 행 삭제\n",
    "    condition = (train_df[\"시술 유형\"] == \"IVF\") & (train_df[\"동결 배아 사용 여부\"] == 0) & (train_df[\"배아 해동 경과일\"].isna()) & (train_df[\"해동된 배아 수\"] != 0)\n",
    "    train_df = train_df[~condition]\n",
    "\n",
    "    # 삭제 후 데이터 크기 저장\n",
    "    train_size_after = len(train_df)\n",
    "\n",
    "    # 삭제된 행 수 계산\n",
    "    train_removed = train_size_before - train_size_after\n",
    "    #   삭제된 행 수 출력\n",
    "    print(f\"  Train 데이터에서 삭제된 행 수: {train_removed}\")\n",
    "\n",
    "    return train_df\n",
    "\n",
    "train_df = remove_embryo_outliers(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_embryo_rows(train_df):\n",
    "    \"\"\"\n",
    "    특정 시술 유형이 IVF인 경우,\n",
    "    '동결 배아 사용 여부' == 0, '해동된 배아 수' == 0, '배아 해동 경과일'이 NaN이 아닌 경우의 행을 삭제하는 함수.\n",
    "    \"\"\"\n",
    "\n",
    "    # 삭제 전 데이터 크기 저장\n",
    "    train_size_before = len(train_df)\n",
    "\n",
    "    #   특정 시술 유형이 IVF인 경우만 적용\n",
    "    condition = (train_df['시술 유형'] == \"IVF\") & \\\n",
    "                (train_df['동결 배아 사용 여부'] == 0) & \\\n",
    "                (train_df['해동된 배아 수'] == 0) & \\\n",
    "                (pd.notna(train_df['배아 해동 경과일']))  #   NaN이 아닌 경우 필터링\n",
    "    train_df = train_df[~condition]  #   조건에 맞는 행 삭제\n",
    "\n",
    "    # 삭제된 행 수 계산\n",
    "    train_removed = train_size_before - len(train_df)\n",
    "\n",
    "    #   삭제된 행 수 출력\n",
    "    print(f\"  Train 데이터에서 삭제된 행 수: {train_removed}\")\n",
    "\n",
    "    return train_df\n",
    "\n",
    "#   함수 사용 예시\n",
    "train_df = remove_invalid_embryo_rows(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_procedure(train_df):\n",
    "    \"\"\"\n",
    "    '특정 시술 유형' 컬럼이 존재할 경우, 결측(NaN)인 행을 제거하고, 제거된 행 수를 출력하는 함수.\n",
    "    컬럼이 없으면 경고 메시지를 출력하고 원래 데이터프레임을 반환.\n",
    "    \"\"\"\n",
    "\n",
    "    # 특정 시술 유형 컬럼 존재 여부 확인\n",
    "    if '특정 시술 유형' not in train_df.columns:\n",
    "        print(\"⚠️ '특정 시술 유형' 컬럼이 데이터프레임에 존재하지 않습니다. 원본 데이터를 반환합니다.\")\n",
    "        return train_df\n",
    "\n",
    "    # 원본 데이터 크기 저장\n",
    "    train_size_before = len(train_df)\n",
    "\n",
    "    # '특정 시술 유형'이 NaN인 행 제거\n",
    "    train_df = train_df.dropna(subset=['특정 시술 유형'])\n",
    "\n",
    "    # 제거된 행 수 계산\n",
    "    train_removed = train_size_before - len(train_df)\n",
    "\n",
    "    # 제거된 행 수 출력\n",
    "    print(f\"  Train 데이터에서 제거된 행 수: {train_removed}\")\n",
    "\n",
    "    return train_df\n",
    "train_df = remove_outliers_procedure(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_embryo_days(train_df, test_df):\n",
    "    \"\"\"\n",
    "    IVF 시술을 받은 환자의 경우,\n",
    "    - '이식된 배아 수'가 0이면 '배아 이식 경과일'의 결측값을 0으로 채움.\n",
    "    - '동결 배아 사용 여부'가 0이면 '배아 해동 경과일'의 결측값을 0으로 채움.\n",
    "\n",
    "    train_df와 test_df 모두 동일한 로직을 적용함.\n",
    "    \"\"\"\n",
    "\n",
    "    def fill_embryo_days(df):\n",
    "        #   IVF 시술을 받은 경우만 적용\n",
    "        ivf_mask = df['시술 유형'] == \"IVF\"\n",
    "\n",
    "        #   '이식된 배아 수' == 0 → '배아 이식 경과일' NaN을 0으로 채우기\n",
    "        condition_1 = ivf_mask & (df['이식된 배아 수'] == 0) & (df['배아 이식 경과일'].isna())\n",
    "        df.loc[condition_1, '배아 이식 경과일'] = 0\n",
    "\n",
    "        #   '동결 배아 사용 여부' == 0 → '배아 해동 경과일' NaN을 0으로 채우기\n",
    "        condition_2 = ivf_mask & (df['동결 배아 사용 여부'] == 0) & (df['배아 해동 경과일'].isna())\n",
    "        df.loc[condition_2, '배아 해동 경과일'] = 0\n",
    "\n",
    "        #   변경된 데이터 개수 출력\n",
    "        print(f\"  [데이터셋: {df.name}] '배아 이식 경과일' 채운 개수: {condition_1.sum()}\")\n",
    "        print(f\"  [데이터셋: {df.name}] '배아 해동 경과일' 채운 개수: {condition_2.sum()}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    #   DataFrame 이름 추가 (출력 시 구분하기 위해)\n",
    "    train_df.name = \"Train\"\n",
    "    test_df.name = \"Test\"\n",
    "\n",
    "    train_df = fill_embryo_days(train_df)\n",
    "    test_df = fill_embryo_days(test_df)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "#   함수 실행\n",
    "train_df, test_df = fill_missing_embryo_days(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape); print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. feature tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Fast Tokenizer 사용하여 속도 향상 및 RAM 절약\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#   컬럼 정의\n",
    "category_columns = [\n",
    "    \"시술 시기 코드\", \"시술 당시 나이\", \"시술 유형\", \"배란 유도 유형\", \"배아 생성 주요 이유\",\n",
    "    \"총 시술 횟수\", \"클리닉 내 총 시술 횟수\", \"IVF 시술 횟수\", \"DI 시술 횟수\",\n",
    "    \"총 임신 횟수\", \"IVF 임신 횟수\", \"DI 임신 횟수\", \"총 출산 횟수\",\n",
    "    \"IVF 출산 횟수\", \"DI 출산 횟수\", \"난자 출처\", \"정자 출처\", \"난자 기증자 나이\", \"정자 기증자 나이\"\n",
    "]\n",
    "\n",
    "binary_columns = [\n",
    "    \"배란 자극 여부\", \"단일 배아 이식 여부\", \"착상 전 유전 검사 사용 여부\", \"착상 전 유전 진단 사용 여부\",\n",
    "    \"남성 주 불임 원인\", \"남성 부 불임 원인\", \"여성 주 불임 원인\", \"여성 부 불임 원인\",\n",
    "    \"부부 주 불임 원인\", \"부부 부 불임 원인\", \"불명확 불임 원인\", \"불임 원인 - 난관 질환\",\n",
    "    \"불임 원인 - 남성 요인\", \"불임 원인 - 배란 장애\", \"불임 원인 - 여성 요인\",\n",
    "    \"불임 원인 - 자궁경부 문제\", \"불임 원인 - 자궁내막증\", \"불임 원인 - 정자 농도\",\n",
    "    \"불임 원인 - 정자 면역학적 요인\", \"불임 원인 - 정자 운동성\", \"불임 원인 - 정자 형태\",\n",
    "    \"동결 배아 사용 여부\", \"신선 배아 사용 여부\", \"기증 배아 사용 여부\", \"대리모 여부\",\n",
    "    \"PGD 시술 여부\", \"PGS 시술 여부\"\n",
    "]\n",
    "\n",
    "numeric_columns = [\n",
    "    \"임신 시도 또는 마지막 임신 경과 연수\", \"총 생성 배아 수\", \"미세주입된 난자 수\",\n",
    "    \"미세주입에서 생성된 배아 수\", \"이식된 배아 수\", \"미세주입 배아 이식 수\",\n",
    "    \"저장된 배아 수\", \"미세주입 후 저장된 배아 수\", \"해동된 배아 수\",\n",
    "    \"해동 난자 수\", \"수집된 신선 난자 수\", \"저장된 신선 난자 수\",\n",
    "    \"혼합된 난자 수\", \"파트너 정자와 혼합된 난자 수\", \"기증자 정자와 혼합된 난자 수\",\n",
    "    \"난자 채취 경과일\", \"난자 해동 경과일\", \"난자 혼합 경과일\",\n",
    "    \"배아 이식 경과일\", \"배아 해동 경과일\"\n",
    "]\n",
    "\n",
    "#   결측값 처리 (RAM 절약)\n",
    "train_df.fillna({\"시술 시기 코드\": \"알 수 없음\", \"배란 자극 여부\": 0}, inplace=True)\n",
    "test_df.fillna({\"시술 시기 코드\": \"알 수 없음\", \"배란 자극 여부\": 0}, inplace=True)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    median_value = train_df[col].median()\n",
    "    train_df[col] = train_df[col].fillna(median_value)\n",
    "    test_df[col] = test_df[col].fillna(median_value)\n",
    "\n",
    "#   연속형 변수 Binning (필요할 때만 실행)\n",
    "for col in numeric_columns:\n",
    "    train_df[col] = pd.qcut(train_df[col], q=10, duplicates=\"drop\", labels=False)\n",
    "    test_df[col] = pd.qcut(test_df[col], q=10, duplicates=\"drop\", labels=False)\n",
    "\n",
    "\n",
    "#   Feature Tokenization 수행\n",
    "def tokenize_features(df):\n",
    "    return [\n",
    "        \", \".join(\n",
    "            [f\"{col}: {row[col]}\" for col in category_columns] +\n",
    "            [f\"{col}: {'True' if row[col] == 1 else 'False'}\" for col in binary_columns] +\n",
    "            [f\"{col}: 구간 {row[col]}\" for col in numeric_columns]\n",
    "        )\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "train_texts = tokenize_features(train_df)\n",
    "test_texts = tokenize_features(test_df)\n",
    "\n",
    "#   Tokenization을 배치 단위로 수행하여 RAM 절약\n",
    "def batch_tokenize(texts, tokenizer, batch_size=128):\n",
    "    input_ids_list, attention_mask_list = [], []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        encodings = tokenizer(batch_texts, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        input_ids_list.append(encodings[\"input_ids\"])\n",
    "        attention_mask_list.append(encodings[\"attention_mask\"])\n",
    "\n",
    "    return torch.cat(input_ids_list), torch.cat(attention_mask_list)\n",
    "\n",
    "train_input_ids, train_attention_masks = batch_tokenize(train_texts, tokenizer, batch_size=128)\n",
    "test_input_ids, test_attention_masks = batch_tokenize(test_texts, tokenizer, batch_size=128)\n",
    "\n",
    "#   NumPy 배열을 사용하여 RAM 절약\n",
    "train_input_ids = train_input_ids.to(torch.int32)\n",
    "test_input_ids = test_input_ids.to(torch.int32)\n",
    "\n",
    "#   불필요한 데이터 삭제하여 RAM 확보\n",
    "del train_texts, test_texts\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Target 데이터 변환 (Test 데이터에는 `labels` 없음!)\n",
    "train_labels = torch.tensor(train_df[\"임신 성공 여부\"].values, dtype=torch.long)\n",
    "\n",
    "#   Tokenized 데이터 저장\n",
    "torch.save({\"input_ids\": train_input_ids, \"attention_mask\": train_attention_masks, \"labels\": train_labels}, dir + \"train_data_fixed.pt\")\n",
    "torch.save({\"input_ids\": test_input_ids, \"attention_mask\": test_attention_masks}, dir + \"test_data_fixed.pt\")  # ❌ labels 없음!\n",
    "\n",
    "print(\"  Tokenized 데이터 저장 완료!\")\n",
    "\n",
    "#   DataLoader 변환\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(\"  Tokenized Train Shape:\", train_input_ids.shape)\n",
    "print(\"  Tokenized Test Shape:\", test_input_ids.shape)\n",
    "print(\"  Train Labels Shape:\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. tokenized 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   저장된 Train 데이터 불러오기\n",
    "train_data = torch.load(dir + \"train_data_fixed.pt\")\n",
    "train_input_ids = train_data[\"input_ids\"]\n",
    "train_attention_masks = train_data[\"attention_mask\"]\n",
    "train_labels = train_data[\"labels\"]\n",
    "\n",
    "#   저장된 Test 데이터 불러오기 (`labels` 없음)\n",
    "test_data = torch.load(dir + \"test_data_fixed.pt\")\n",
    "test_input_ids = test_data[\"input_ids\"]\n",
    "test_attention_masks = test_data[\"attention_mask\"]\n",
    "\n",
    "print(\"  저장된 Tokenized 데이터 불러오기 완료!\")\n",
    "\n",
    "#   Train DataLoader 생성\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "#   Test DataLoader 생성 (labels 없음!)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"  DataLoader 생성 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trian validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Train / Validation 분할 (80:20)\n",
    "train_input_ids, val_input_ids, train_attention_masks, val_attention_masks, train_labels, val_labels = train_test_split(\n",
    "    train_data[\"input_ids\"], train_data[\"attention_mask\"], train_data[\"labels\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"  Train/Validation 데이터 분할 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. 모델 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#   GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#   DistilBERT 모델 사용 (속도 2배 증가)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "model.to(device)\n",
    "model.gradient_checkpointing_enable()  #   VRAM 절약\n",
    "\n",
    "#   옵티마이저 및 스케줄러\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "num_training_steps = len(train_dataloader) * 5\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "#   Mixed Precision Training (AMP)\n",
    "scaler = GradScaler()\n",
    "\n",
    "#   학습 파라미터\n",
    "EPOCHS = 5\n",
    "best_auc = 0.0\n",
    "gradient_accumulation_steps = 4  #   작은 배치를 모아서 업데이트\n",
    "\n",
    "#   학습 루프\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Training Loop\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS} Training\")):\n",
    "        input_ids, attention_mask, labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss / gradient_accumulation_steps  #   작은 배치 단위로 나눔\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:  #   배치 누적 후 최적화 실행\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Train Loss: {train_loss / len(train_dataloader):.4f}\")\n",
    "\n",
    "    #   Validation 단계 (AUC 계산)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    true_labels = []\n",
    "    pred_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS} Validation\"):\n",
    "            input_ids, attention_mask, labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            #   AUC 점수 계산을 위한 확률값 저장\n",
    "            probs = torch.softmax(logits, dim=1)[:, 1]  #   임신 성공 (1)의 확률\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_auc = roc_auc_score(true_labels, pred_probs)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}, AUC: {val_auc:.4f}\")\n",
    "\n",
    "    #   AUC가 가장 높은 모델 저장\n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        torch.save(model.state_dict(), dir_models + \"best_bert_model_fixed.pth\")\n",
    "        print(\"  Best Model Saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1. train 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   저장된 Train 데이터 불러오기\n",
    "train_data = torch.load(dir + \"train_data_fixed.pt\")\n",
    "train_input_ids = train_data[\"input_ids\"]\n",
    "train_attention_masks = train_data[\"attention_mask\"]\n",
    "train_labels = train_data[\"labels\"]\n",
    "\n",
    "#   저장된 Test 데이터 불러오기 (`labels` 없음)\n",
    "test_data = torch.load(dir + \"test_data_fixed.pt\")\n",
    "test_input_ids = test_data[\"input_ids\"]\n",
    "test_attention_masks = test_data[\"attention_mask\"]\n",
    "\n",
    "print(\"  저장된 Tokenized 데이터 불러오기 완료!\")\n",
    "\n",
    "#   Train DataLoader 생성\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "#   Test DataLoader 생성 (labels 없음!)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"  DataLoader 생성 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#   저장된 모델 불러오기\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "model.load_state_dict(torch.load(dir_models + \"best_bert_model_fixed.pth\", map_location=device))  # 모델 가중치 로드\n",
    "model.to(device)\n",
    "model.eval()  # 평가 모드 설정\n",
    "\n",
    "print(\"  최적의 DistilBERT 모델 불러오기 완료!\")\n",
    "\n",
    "#   Train 데이터 예측 함수\n",
    "def predict(model, dataloader):\n",
    "    predictions = []\n",
    "    pred_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            probs = torch.softmax(logits, dim=1)  #   확률값 (Softmax)\n",
    "            preds = torch.argmax(probs, dim=1)  #   최종 예측값 (0 or 1)\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            pred_probs.extend(probs[:, 1].cpu().numpy())  #   \"임신 성공(1)\" 확률 저장\n",
    "\n",
    "    return np.array(predictions), np.array(pred_probs)\n",
    "\n",
    "#   Train 데이터 예측 수행\n",
    "train_predictions, train_probabilities = predict(model, train_dataloader)\n",
    "\n",
    "print(\"  Train 데이터 예측 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 예측 저장\n",
    "pd.DataFrame({'predictions':train_predictions, 'probability': train_probabilities}).to_csv(dir_to_submit + \"train_BERT_probability_fixed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2. test 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#   저장된 모델 경로\n",
    "best_model_path = dir_models + \"best_bert_model_fixed.pth\"  # 저장된 모델 파일\n",
    "\n",
    "#   DistilBERT 모델 불러오기 (저장된 모델과 동일한 모델 사용)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))  # 가중치 로드\n",
    "model.to(device)\n",
    "model.eval()  # 평가 모드 설정\n",
    "\n",
    "print(\"  최적의 DistilBERT 모델 불러오기 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   저장된 Test 데이터 불러오기 (labels 없음!)\n",
    "test_data = torch.load(dir + \"test_data_fixed.pt\")\n",
    "test_input_ids = test_data[\"input_ids\"]\n",
    "test_attention_masks = test_data[\"attention_mask\"]\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"  Test 데이터 불러오기 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#   테스트 데이터 예측 함수\n",
    "def predict(model, dataloader):\n",
    "    predictions = []\n",
    "    pred_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            probs = torch.softmax(logits, dim=1)  #   확률값 (Softmax)\n",
    "            preds = torch.argmax(probs, dim=1)  #   최종 예측값 (0 or 1)\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            pred_probs.extend(probs[:, 1].cpu().numpy())  #   \"임신 성공\"일 확률 저장\n",
    "\n",
    "    return np.array(predictions), np.array(pred_probs)\n",
    "\n",
    "#   Test 데이터 예측 수행\n",
    "test_predictions, test_probabilities = predict(model, test_dataloader)\n",
    "\n",
    "print(\"  Test 데이터 예측 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(dir + 'sample_submission.csv', encoding = 'utf-8')\n",
    "sample_submission['predictions'] = test_predictions\n",
    "sample_submission['probability'] = test_probabilities\n",
    "display(sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(dir_to_submit + \"BERT_probability_fixed.csv\", encoding= 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Stacking \n",
    "\n",
    "* Base Model : CatBoost, XgBoost, BERT\n",
    "\n",
    "    * Base Model 1 : CatBoost\n",
    "\n",
    "    * Base Model 2 : XgBoost\n",
    "\n",
    "    * Base Model 3 : BERT\n",
    "\n",
    "* Meta Model : Logistic Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data load\n",
    "train_cat = pd.read_csv('./submit/train_pred_cat.csv', encoding='utf-8').iloc[:,1]\n",
    "train_xgb = pd.read_csv('./submit/train_pred_xgb.csv', encoding='utf-8').iloc[:,1]\n",
    "train_bert = pd.read_csv('./submit/train_BERT_probability_fixed.csv', encoding='utf-8')['probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data load\n",
    "test_cat = pd.read_csv('./submit/test_pred_cat.csv', encoding = 'utf-8').iloc[:,1]\n",
    "test_xgb = pd.read_csv('./submit/test_pred_xgb.csv', encoding = 'utf-8').iloc[:,1]\n",
    "test_bert = pd.read_csv('./submit/BERT_probability_fixed.csv', encoding = 'utf-8')['probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape 확인 (254733,)(90067,)\n",
    "print(train_cat.shape,end = ''); print(test_cat.shape)\n",
    "print(train_xgb.shape,end = ''); print(test_xgb.shape)\n",
    "print(train_bert.shape,end = ''); print(test_bert.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack : catboost, xgboost, bert\n",
    "stack_train = np.column_stack((train_cat,train_xgb,train_bert))\n",
    "stack_test = np.column_stack((test_cat,test_xgb, test_bert))\n",
    "print(stack_train.shape)\n",
    "print(stack_test.shape)\n",
    "\n",
    "#   메타 모델 (Logistic Regression) 학습\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(stack_train, y)\n",
    "\n",
    "#   최종 예측 수행\n",
    "final_predictions = meta_model.predict_proba(stack_test)[:, 1]\n",
    "\n",
    "#   예측값 출력\n",
    "print(\"\\n  stack_test Predictions Completed!\")\n",
    "print(final_predictions[:10])  # 앞 10개 예측값 출력 (샘플)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(dir + 'sample_submission.csv', encoding = 'utf-8')\n",
    "sample_submission['probability'] = final_predictions\n",
    "display(sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출할 파일명\n",
    "file_name = dir_to_submit + \"final_submission.csv\"\n",
    "sample_submission.to_csv(file_name, index=False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnflwlq145",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
